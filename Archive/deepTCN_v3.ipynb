{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F\n",
    "import pandas as pd  \n",
    "from data_process import *\n",
    "import torch.optim as optim \n",
    "from tqdm import tqdm \n",
    "from utils import plot_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.dilated_conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=dilation, dilation=dilation)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dilated_conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=dilation, dilation=dilation)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.dilated_conv1(x)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dilated_conv2(out)\n",
    "        out = self.batch_norm2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        return self.relu(out + residual)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dense1 = nn.Linear(in_channels, out_channels)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(out_channels)\n",
    "        self.dense2 = nn.Linear(out_channels, out_channels)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dense1(x)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dense2(out)\n",
    "        out = self.batch_norm2(out)\n",
    "        return out\n",
    "\n",
    "class DeepTCN(nn.Module):\n",
    "    def __init__(self, num_series, num_outputs, num_blocks, kernel_size, hidden_channels, num_covariates):\n",
    "        super(DeepTCN, self).__init__()\n",
    "        self.num_covariates = num_covariates\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_series if i == 0 else hidden_channels\n",
    "            layers += [ResidualBlock(in_channels, hidden_channels, kernel_size, dilation_size)]\n",
    "\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        self.decoder = Decoder(hidden_channels + num_covariates, hidden_channels)\n",
    "        self.output_layer = nn.Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, covariates):\n",
    "        # Encoder\n",
    "        x = x.permute(0, 2, 1)  # Assuming input of shape [batch, seq_len, features]\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # Decoder\n",
    "        x = x.mean(dim=2)  # Global average pooling\n",
    "        #x = x.unsqueeze(1)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for seq_len_idx in range(covariates.shape[1]):\n",
    "            current_covariate = covariates[:, seq_len_idx, :]\n",
    "            combined = torch.cat((x, current_covariate), dim=1) \n",
    "            decoded = self.decoder(combined)\n",
    "            outputs.append(decoded)\n",
    "        \n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        outputs = self.output_layer(outputs)\n",
    "        outputs = outputs.squeeze(-1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "flights = pd.read_csv(\"flights.csv\")\n",
    "features = getFeatures() \n",
    "data, train_loader, val_loader, test_loader, d_split = get_data_loaders(flights, covariates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\n# Training loop\\nnum_epochs = 20\\nfor epoch in range(num_epochs):\\n    model.train()  # Set the model to training mode\\n    total_loss = 0\\n    train_loader_with_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\")\\n    \\n    for batch_idx, (features, targets) in enumerate(train_loader_with_progress):\\n        # Separate historical features and covariates and move to device\\n        historical_features = features[:, :, :-6].to(device)\\n        covariates = features[:, :, -6:].to(device)\\n        targets = targets.to(device)\\n        \\n        # Forward pass\\n        outputs = model(historical_features, covariates)\\n        loss = criterion(outputs, targets)\\n        \\n        # Backward pass and optimization\\n        optimizer.zero_grad()\\n        loss.backward() \\n        optimizer.step() \\n        \\n        total_loss += loss.item() \\n        train_loader_with_progress.set_postfix({\\'Train Loss\\': loss.item()})\\n        \\n    print(f\\'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}\\')\\n    train_losses.append(total_loss/len(train_loader))\\n    \\n    # Validation loop\\n    model.eval()  # Set the model to evaluation mode\\n    val_loss = 0\\n    val_loader_with_progress = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Validation\")\\n    with torch.no_grad():  # Disable gradient calculation\\n        for val_features, val_targets in val_loader_with_progress:\\n            val_historical_features = val_features[:, :, :-6].to(device)\\n            val_covariates = val_features[:, :, -6:].to(device)\\n            val_targets = val_targets.to(device)\\n\\n            val_outputs = model(val_historical_features, val_covariates)\\n            val_loss += criterion(val_outputs, val_targets).item()\\n            val_loader_with_progress.set_postfix({\\'Val Loss\\': criterion(val_outputs, val_targets).item()})\\n        \\n        val_loss /= len(val_loader)\\n        val_losses.append(val_loss)\\n    print(f\\'Validation Loss: {val_loss}\\')\\n    \\n    # Save the model with least validation loss\\n    if val_loss < best_val_loss:\\n        best_epoch = epoch + 1\\n        best_val_loss = val_loss\\n        torch.save(model.state_dict(), f\\'DeepTCN_model_epoch_{best_epoch}_val_loss_{best_val_loss:.4f}.pt\\')\\n\\n# After training\\nprint(f\\'Best Validation Loss: {best_val_loss} at Epoch: {best_epoch}\\')\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_series = 32  # Historical features\n",
    "num_covariates = 6  # Future covariates\n",
    "num_outputs = 2  # Output sequence length\n",
    "num_blocks = 4  # Number of residual blocks\n",
    "kernel_size = 3  # Kernel size\n",
    "hidden_channels = 64  # Number of filters in convolutional layers\n",
    "\n",
    "model = DeepTCN(num_series=num_series, num_outputs=num_outputs, num_blocks=num_blocks,\n",
    "                kernel_size=kernel_size, hidden_channels=hidden_channels, num_covariates=num_covariates)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "best_val_loss = float('inf')\n",
    "train_losses, val_losses = [], []\n",
    "best_epoch = 0\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    train_loader_with_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\")\n",
    "\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader_with_progress):\n",
    "        #print(features.shape)\n",
    "        # Separate historical features and covariates and move to device\n",
    "        historical_features = features[:, :, :-6].to(device)\n",
    "        covariates = features[:, :num_outputs, -6:].to(device)\n",
    "\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(historical_features, covariates)\n",
    "\n",
    "        #print(\"Outputs shape:\", outputs.shape)\n",
    "        #print(\"Targets shape:\", targets.shape)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        train_loader_with_progress.set_postfix({'Train Loss': loss.item()})\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}')\n",
    "    train_losses.append(total_loss/len(train_loader))\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    val_loader_with_progress = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Validation\")\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for val_features, val_targets in val_loader_with_progress:\n",
    "            val_historical_features = val_features[:, :, :-6].to(device)\n",
    "            val_covariates = val_features[:, :num_outputs, -6:].to(device)\n",
    "            val_targets = val_targets.to(device)\n",
    "\n",
    "            val_outputs = model(val_historical_features, val_covariates)\n",
    "            val_loss += criterion(val_outputs, val_targets).item()\n",
    "            val_loader_with_progress.set_postfix({'Val Loss': criterion(val_outputs, val_targets).item()})\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "    print(f'Validation Loss: {val_loss}')\n",
    "\n",
    "    # Save the model with least validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_epoch = epoch + 1\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), f'TCN_model_epoch_{best_epoch}_val_loss_{best_val_loss:.4f}.pt')\n",
    "\n",
    "# After training\n",
    "print(f'Best Validation Loss: {best_val_loss} at Epoch: {best_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_output_v1(y_pred_seq, y_true_seq, focus_length=100):\n",
    "    pred_arr = y_pred_seq[:focus_length].cpu().numpy().flatten()\n",
    "    true_arr = y_true_seq[:focus_length].cpu().numpy().flatten()\n",
    "\n",
    "    # Set up subplots\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "\n",
    "    # Plot predictions and actual values\n",
    "    axs[0].plot(pred_arr, 'r-', label='Predicted', linewidth=2.0)\n",
    "    axs[0].plot(true_arr, 'b-', label='Actual', linewidth=2.0)\n",
    "    axs[0].set_title('Predicted vs Actual Power Consumption')\n",
    "    axs[0].set_ylabel('Power Consumption')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # Plot the difference\n",
    "    difference = np.abs(pred_arr - true_arr)\n",
    "    axs[1].plot(difference, 'k--', label='Difference', linewidth=2.0)\n",
    "    axs[1].set_title('Difference in Power Consumption')\n",
    "    axs[1].set_ylabel('Absolute Difference')\n",
    "    axs[1].set_xlabel('Sequence Index')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def plot_output_v2(y_pred_seq, y_true_seq, seq_len, focus_length=100):\n",
    "    # Determine how many full sequences fit within the focus_length\n",
    "    num_sequences = focus_length // seq_len\n",
    "    \n",
    "    pred_arr = y_pred_seq[:num_sequences].cpu().numpy().reshape(-1, seq_len).flatten()\n",
    "    true_arr = y_true_seq[:num_sequences].cpu().numpy().reshape(-1, seq_len).flatten()\n",
    "\n",
    "    # Set up subplots\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "\n",
    "    # Plot predictions and actual values\n",
    "    for i in range(num_sequences):\n",
    "        axs[0].plot(range(i*seq_len, (i+1)*seq_len), pred_arr[i*seq_len:(i+1)*seq_len], 'r-', linewidth=2.0)\n",
    "        axs[0].plot(range(i*seq_len, (i+1)*seq_len), true_arr[i*seq_len:(i+1)*seq_len], 'b-', linewidth=2.0)\n",
    "\n",
    "    axs[0].set_title('Predicted vs Actual Power Consumption')\n",
    "    axs[0].set_ylabel('Power Consumption')\n",
    "    axs[0].legend(['Predicted', 'Actual'])\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # Plot the difference\n",
    "    difference = np.abs(pred_arr - true_arr)\n",
    "    for i in range(num_sequences):\n",
    "        axs[1].plot(range(i*seq_len, (i+1)*seq_len), difference[i*seq_len:(i+1)*seq_len], 'k--', linewidth=2.0)\n",
    "\n",
    "    axs[1].set_title('Difference in Power Consumption')\n",
    "    axs[1].set_ylabel('Absolute Difference')\n",
    "    axs[1].set_xlabel('Sequence Index')\n",
    "    axs[1].legend(['Difference'])\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "38\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/lianyang/Desktop/UMich/Fall 2023/EECS 553/Project/deepTCN_v3.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lianyang/Desktop/UMich/Fall%202023/EECS%20553/Project/deepTCN_v3.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m historical_features \u001b[39m=\u001b[39m input_seq[:, :, :\u001b[39m-\u001b[39m\u001b[39m6\u001b[39m]  \u001b[39m# All but the last 6 columns\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lianyang/Desktop/UMich/Fall%202023/EECS%20553/Project/deepTCN_v3.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m covariates \u001b[39m=\u001b[39m input_seq[:, :, \u001b[39m-\u001b[39m\u001b[39m6\u001b[39m:]          \u001b[39m# Only the last 6 columns\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lianyang/Desktop/UMich/Fall%202023/EECS%20553/Project/deepTCN_v3.ipynb#W4sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(historical_features, covariates)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lianyang/Desktop/UMich/Fall%202023/EECS%20553/Project/deepTCN_v3.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# Adjust dimensions if necessary\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lianyang/Desktop/UMich/Fall%202023/EECS%20553/Project/deepTCN_v3.ipynb#W4sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m targets \u001b[39m=\u001b[39m output_seq\n",
      "File \u001b[0;32m~/Desktop/UMich/Fall 2023/EECS 553/Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/UMich/Fall 2023/EECS 553/Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/lianyang/Desktop/UMich/Fall 2023/EECS 553/Project/deepTCN_v3.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lianyang/Desktop/UMich/Fall%202023/EECS%20553/Project/deepTCN_v3.ipynb#W4sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, covariates):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lianyang/Desktop/UMich/Fall%202023/EECS%20553/Project/deepTCN_v3.ipynb#W4sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39m# Encoder\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lianyang/Desktop/UMich/Fall%202023/EECS%20553/Project/deepTCN_v3.ipynb#W4sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# Assuming input of shape [batch, seq_len, features]\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lianyang/Desktop/UMich/Fall%202023/EECS%20553/Project/deepTCN_v3.ipynb#W4sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lianyang/Desktop/UMich/Fall%202023/EECS%20553/Project/deepTCN_v3.ipynb#W4sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     \u001b[39m# Decoder\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lianyang/Desktop/UMich/Fall%202023/EECS%20553/Project/deepTCN_v3.ipynb#W4sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)  \u001b[39m# Global average pooling\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/UMich/Fall 2023/EECS 553/Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/UMich/Fall 2023/EECS 553/Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/UMich/Fall 2023/EECS 553/Project/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/UMich/Fall 2023/EECS 553/Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/UMich/Fall 2023/EECS 553/Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/lianyang/Desktop/UMich/Fall 2023/EECS 553/Project/deepTCN_v3.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lianyang/Desktop/UMich/Fall%202023/EECS%20553/Project/deepTCN_v3.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lianyang/Desktop/UMich/Fall%202023/EECS%20553/Project/deepTCN_v3.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     residual \u001b[39m=\u001b[39m x\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lianyang/Desktop/UMich/Fall%202023/EECS%20553/Project/deepTCN_v3.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilated_conv1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lianyang/Desktop/UMich/Fall%202023/EECS%20553/Project/deepTCN_v3.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_norm1(out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lianyang/Desktop/UMich/Fall%202023/EECS%20553/Project/deepTCN_v3.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/Desktop/UMich/Fall 2023/EECS 553/Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/UMich/Fall 2023/EECS 553/Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/UMich/Fall 2023/EECS 553/Project/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/Desktop/UMich/Fall 2023/EECS 553/Project/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    307\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.load_state_dict(torch.load('/content/TCN_model_epoch_13_val_loss_0.0017.pt', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "features = getFeatures(covariates=True)\n",
    "print(\"features length = \", len(features))\n",
    "\n",
    "flights_nums = d_split['test']\n",
    "flight_num = random.choice(flights_nums)\n",
    "\n",
    "flight_num = 276\n",
    "\n",
    "print(\"flight_num = \", flight_num)\n",
    "test_data = data[data['flight'] == flight_num]\n",
    "\n",
    "#def create_sequences(input_data, output_data, input_seq_len, output_seq_len):\n",
    "\n",
    "# Generate sequences\n",
    "input_seq, output_seq = create_sequences(input_data=test_data[features].values, output_data=test_data['power'].values, input_seq_len=10, output_seq_len=2)\n",
    "\n",
    "# Convert sequences to tensors and move to device\n",
    "input_seq = torch.tensor(input_seq, dtype=torch.float32).to('cuda')  # Replace 'cpu' with 'cuda' if using GPU\n",
    "output_seq = torch.tensor(output_seq, dtype=torch.float32).to('cuda')\n",
    "\n",
    "mape_list = []\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    historical_features = input_seq[:, :, :-6]  # All but the last 6 columns\n",
    "    covariates = input_seq[:, :num_outputs, -6:]          # Only the last 6 columns\n",
    "\n",
    "    outputs = model(historical_features, covariates)\n",
    "\n",
    "    targets = output_seq\n",
    "\n",
    "    # Calculate error metrics\n",
    "    error = nn.MSELoss()(outputs, targets)\n",
    "    print(f\"Test loss: {error}\")\n",
    "\n",
    "    mape = torch.mean(torch.abs((targets - outputs) / (targets + 1e-8))) * 100\n",
    "    print(f\"MAPE: {mape.item()}\")\n",
    "    mape_list.append(mape)\n",
    "\n",
    "    # Call the function with the outputs and targets\n",
    "    #plot_output(outputs, targets, seq_len=2, focus_length=100)\n",
    "    plot_output(outputs.cpu(), targets.cpu(), num_outputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
